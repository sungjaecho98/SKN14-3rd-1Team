# -*- coding: utf-8 -*-
"""data_refine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qO5cmLv-4W7TBNXBW1BOdGKwpSktEsLa
"""

# !pip install langchain langchain-openai langchain-pinecone langchain-community

#from google.colab import userdata
from langchain.prompts import PromptTemplate
from openai import OpenAI
import dotenv
from pinecone import Pinecone
from config import load_config
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Pinecone
import os, json
from langchain.schema import Document
from langchain_pinecone import PineconeVectorStore
from langchain_openai import OpenAIEmbeddings
from pinecone import Pinecone, ServerlessSpec
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain



cfg = load_config()
pc = Pinecone(api_key=cfg['PINECONE_API_KEY'], environment=cfg['PINECONE_ENV'])
embeddings = OpenAIEmbeddings(model=cfg['OPENAI_EMBEDDING_MODEL'])

"""## API -> ë¬¸ì„œ"""

import requests
from langchain_core.documents import Document

def json_to_documents(api_json: dict) -> list[Document]:
    documents = []

    for entry in api_json['body']['items']:
        data = entry['item']

        # Helper function to get and strip a value, handling None
        def get_and_strip(data_dict, key):
            value = data_dict.get(key)
            return value.strip() if isinstance(value, str) else ''

        # page_content êµ¬ì„±
        text = f"""
ì œí’ˆëª…: {get_and_strip(data, 'PRDUCT')}
ì œì¡°ì‚¬: {get_and_strip(data, 'ENTRPS')}
ê¸°ëŠ¥ì„±: {get_and_strip(data, 'MAIN_FNCTN')}
ì„­ì·¨ ì‹œ ì£¼ì˜ì‚¬í•­: {get_and_strip(data, 'INTAKE_HINT1')}
ë³´ê´€ì¡°ê±´: {get_and_strip(data, 'PRSRV_PD')}
ìœ í†µê¸°í•œ: {get_and_strip(data, 'DISTB_PD')}
"""

        # metadata êµ¬ì„±
        metadata = {
            "ë“±ë¡ì¼ì": data.get("STTEMNT_NO"),
            "ì œì¡°ì‚¬": data.get("ENTRPS"),
            "ê¸°ì¤€ê·œê²©": get_and_strip(data, "BASE_STANDARD")
        }

        # Document ìƒì„±
        documents.append(Document(page_content=text, metadata=metadata))

    return documents


def fetch_all_documents(api_url, api_key, num_of_rows=100) -> list[Document]:
    all_documents = []

    # ë¨¼ì € 1í˜ì´ì§€ í˜¸ì¶œ
    params = {
        "ServiceKey": api_key,
        "pageNo": "1",
        "numOfRows": str(num_of_rows),
        "type": "json",
    }

    response = requests.get(api_url, params=params, timeout=10)
    response.raise_for_status()
    first_page = response.json()

    total_count = int(first_page['body']['totalCount'])
    total_pages = (total_count // num_of_rows) + (1 if total_count % num_of_rows else 0)

    # 1í˜ì´ì§€ â†’ Document ì¶”ì¶œ
    all_documents.extend(json_to_documents(first_page))

    # 2í˜ì´ì§€ë¶€í„° ë§ˆì§€ë§‰ í˜ì´ì§€ê¹Œì§€ ë°˜ë³µ
    for page in range(2, total_pages + 1):
        params['pageNo'] = str(page)
        response = requests.get(api_url, params=params, timeout=10)
        response.raise_for_status()
        page_json = response.json()

        docs = json_to_documents(page_json)
        all_documents.extend(docs)
        print(f"ğŸ“„ {page}/{total_pages} í˜ì´ì§€ ìˆ˜ì§‘ ì™„ë£Œ")

    print(f"\nâœ… ì´ {len(all_documents)}ê°œì˜ Document ê°ì²´ ìƒì„± ì™„ë£Œ")
    return all_documents

url = 'http://apis.data.go.kr/1471000/HtfsInfoService03/getHtfsItem01'
api_key = 'nsfVX4dKQRFTeyldmuRefFQqL8xOsDkkyw8TsU4dA4fO9vq7Zl7JTbrakHnVYBqRG62CWBhhOVwBaGgCBbm3AA=='

documents = fetch_all_documents(url, api_key)
print(documents[0].page_content)
print(documents[0].metadata)

print(documents[41000].page_content)
print(documents[41000].metadata)

"""## ë¬¸ì„œ -> ë²¡í„°ìŠ¤í† ì–´"""

# ë¬¸ì„œë¡œë¶€í„° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pinecone import Pinecone
import time
import os


# ì„ë² ë”© ëª¨ë¸
embeddings = OpenAIEmbeddings(model=os.environ['OPENAI_EMBEDDING_MODEL'])

# ë¬¸ì„œ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
split_documents = text_splitter.split_documents(documents)

# Pinecone ìƒì„±
pinecone_api_key = os.environ.get('PINECONE_API_KEY')

pc = Pinecone(api_key=pinecone_api_key)

index_name = 'health-supplement-rag'

# index ë§Œë“¤ì–´ë…¼ê±° ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=embeddings.get_dimension(),
        metric='cosine'
    )
    #
    while not pc.describe_index(index_name).status['ready']:
        time.sleep(1)

vector_store = PineconeVectorStore(index_name=index_name, embedding=embeddings)

# ë°°ì¹˜ë¡œ ë°€ì–´ ë„£ê¸°
batch_size = 100  # ë°°ì¹˜ì‚¬ì´ì¦ˆ ì§€ì •
for i in range(0, len(split_documents), batch_size):
    batch = split_documents[i:i + batch_size]
    # vector_store.add_documents(batch)
    print(f"Added batch {i//batch_size + 1}/{(len(split_documents)//batch_size) + 1}")

print("\nâœ… All documents added to Pinecone vector store.")

# ì¡´ì¬í•˜ëŠ” ì¸ë±ìŠ¤ì— ì ‘ê·¼/ê²€ìƒ‰
from pprint import pprint

retriever = vector_store.as_retriever(
    search_type='similarity',
    search_kwargs={'k': 3} # ìœ ì‚¬í•œ ë¬¸ì„œ 3ê°œê¹Œì§€ ê²€ìƒ‰
)
pprint(retriever.invoke('í”¼ë¡œê°œì„ ì— ë„ì›€ì´ ë˜ëŠ” ì˜ì–‘ì œëŠ”?'))

"""## RAG êµ¬ì„± (Retriever + LLM â†’ ìµœì¢… ë‹µë³€ ìƒì„±) ë‹¨ê³„"""

# 1. RAGìš© í”„ë¡¬í”„íŠ¸
rag_prompt = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ë° ì˜ì–‘ì œ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ê´€ë ¨ ë¬¸ì„œì…ë‹ˆë‹¤:

{context}

ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ ì§ˆë¬¸ì— ìµœëŒ€í•œ ì •í™•í•˜ê³  ì‹ ë¢°ì„± ìˆê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì§ˆë¬¸: "{question}"

- ë°˜ë“œì‹œ ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
- íš¨ê³¼ë‚˜ íš¨ëŠ¥ì´ ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
- ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ê±´ê°•ê¸°ëŠ¥ì‹í’ˆì„ êµ¬ì²´ì ìœ¼ë¡œ ì¶”ì²œí•˜ì„¸ìš”.
- ë§íˆ¬ëŠ” ì¹œì ˆí•˜ê³  ìƒëƒ¥í•˜ë˜, ì •ë³´ëŠ” ì •í™•í•˜ê²Œ ì œê³µí•˜ì„¸ìš”.

â€» ë‹µë³€ ë§ˆì§€ë§‰ì— ë‹¤ìŒ ë¬¸ì¥ì„ ë¶™ì´ì„¸ìš”:
ê±´ê°•ê¸°ëŠ¥ì‹í’ˆì€ ì˜ì•½í’ˆì´ ì•„ë‹™ë‹ˆë‹¤. ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.
""")

# 2. fallback í”„ë¡¬í”„íŠ¸ (ê²€ìƒ‰ ê²°ê³¼ ì—†ì„ ë•Œ)
fallback_prompt = PromptTemplate.from_template("""
"{question}"ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê¸° ì–´ë ¤ìš°ë¯€ë¡œ, ì„£ë¶€ë¥¸ ì¶”ì²œì€ í•˜ì§€ ì•Šê² ìŠµë‹ˆë‹¤.

ê±´ê°•ê¸°ëŠ¥ì‹í’ˆì€ ì˜ì•½í’ˆì´ ì•„ë‹™ë‹ˆë‹¤. ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.
""")

# 3. LLM ì„¸íŒ…
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
    model_name="gpt-4.1-mini",
    temperature=0.3,
    max_tokens=512
)

# 4. QA ì²´ì¸ (stuff ë°©ì‹)
from langchain.chains.question_answering import load_qa_chain
qa_chain = load_qa_chain(llm=llm, chain_type="stuff", prompt=rag_prompt)

# 5. fallback ì²´ì¸
fallback_chain = LLMChain(llm=llm, prompt=fallback_prompt)

# 6. ìµœì¢… ì‹¤í–‰ ì²´ì¸ (ë¬¸ì„œ ê²€ìƒ‰ â†’ ìˆìœ¼ë©´ QA, ì—†ìœ¼ë©´ fallback)
def custom_rag_executor(question: str):
    # ë¬¸ì„œ ê²€ìƒ‰
    docs = retriever.get_relevant_documents(question)

    if docs:
        # ê²€ìƒ‰ ê²°ê³¼ ìˆìŒ â†’ QA ìˆ˜í–‰
        return qa_chain.run(input_documents=docs, question=question)
    else:
        # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â†’ fallback ìˆ˜í–‰
        return fallback_chain.run({"question": question})

# 7. ì§ˆë¬¸ ì‹¤í–‰
query = "ë˜¥ì´ ì•ˆ ë‚˜ì™€"
response = custom_rag_executor(query)
print("\nì±—ë´‡ ì‘ë‹µ:")
print(response)
