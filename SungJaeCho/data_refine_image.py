# SungJaeCho/data_refine.py

import os
import time
import requests
from pprint import pprint
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_pinecone import PineconeVectorStore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pinecone import Pinecone
from langchain.chains import RetrievalQA

# ------------------------
# 1. API ì‘ë‹µ -> LangChain Document ë³€í™˜ í•¨ìˆ˜
# ------------------------
def json_to_documents(api_json: dict) -> list[Document]:
    documents = []

    # ë„¤ì´ë²„ API í‚¤ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
    naver_client_id = os.environ.get("NAVER_CLIENT_ID")
    naver_client_secret = os.environ.get("NAVER_CLIENT_SECRET")

    for entry in api_json['body']['items']:
        data = entry['item']

        def get_and_strip(data_dict, key):
            value = data_dict.get(key)
            return value.strip() if isinstance(value, str) else ''

        product_name = get_and_strip(data, 'PRDUCT')
        image_url = ""
        if naver_client_id and naver_client_secret and product_name:
            image_url = fetch_image_url(product_name, naver_client_id, naver_client_secret)

        text = f"""
ì œí’ˆëª…: {product_name}
ì œì¡°ì‚¬: {get_and_strip(data, 'ENTRPS')}
ê¸°ëŠ¥ì„±: {get_and_strip(data, 'MAIN_FNCTN')}
ì„­ì·¨ ì‹œ ì£¼ì˜ì‚¬í•­: {get_and_strip(data, 'INTAKE_HINT1')}
ë³´ê´€ì¡°ê±´: {get_and_strip(data, 'PRSRV_PD')}
ìœ í†µê¸°í•œ: {get_and_strip(data, 'DISTB_PD')}
"""

        metadata = {
            "ì œí’ˆëª…": product_name,
            "ë“±ë¡ì¼ì": data.get("STTEMNT_NO"),
            "ì œì¡°ì‚¬": data.get("ENTRPS"),
            "ê¸°ì¤€ê·œê²©": get_and_strip(data, "BASE_STANDARD"),
            "ì´ë¯¸ì§€URL": image_url
        }

        documents.append(Document(page_content=text, metadata=metadata))

    return documents

# ------------------------
# 2. ì „ì²´ í˜ì´ì§€ ë°˜ë³µ ìš”ì²­í•˜ì—¬ ëª¨ë“  ë¬¸ì„œ ìˆ˜ì§‘
# ------------------------
# def fetch_all_documents(api_url, api_key, num_of_rows=100) -> list[Document]:
#     all_documents = []
#     params = {
#         "ServiceKey": api_key, # API í‚¤
#         "pageNo": "1", # ì‹œì‘ í˜ì´ì§€
#         "numOfRows": str(num_of_rows), # í•œ í˜ì´ì§€ì— ìˆ˜ì§‘í•  ë¬¸ì„œ ìˆ˜
#         "type": "json", # ì‘ë‹µ í˜•ì‹
#     } # API ìš”ì²­ íŒŒë¼ë¯¸í„° ì„¤ì •

#     # ì¼ë‹¨ ì²« í˜ì´ì§€ ìš”ì²­
#     response = requests.get(api_url, params=params, timeout=10) # API ìš”ì²­
#     response.raise_for_status() # ì‘ë‹µ ìƒíƒœ ì½”ë“œ í™•ì¸
#     first_page = response.json() # ì²« í˜ì´ì§€ ì‘ë‹µ JSON íŒŒì‹±

#     total_count = int(first_page['body']['totalCount']) # ì „ì²´ ë¬¸ì„œ ìˆ˜ ì¹´ìš´íŠ¸, 1í˜ì´ì§€ ì‘ë‹µì„ í†µí•´ ì „ì²´ ë¬¸ì„œ ìˆ˜ íŒŒì•…
#     total_pages = (total_count // num_of_rows) + (1 if total_count % num_of_rows else 0) # ì „ì²´ í˜ì´ì§€ ìˆ˜ ê³„ì‚°

#     # ì´ë¯¸ ë°›ì€ first_pageë¥¼ í™œìš©í•´ì„œ ë¬¸ì„œë¡œ ë°”ê¾¸ê³  ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
#     all_documents.extend(json_to_documents(first_page)) # .extend(docs)ëŠ” all_documentsì— ë¬¸ì„œë“¤ì„ ë‚±ê°œë¡œ ì°¨ê³¡ì°¨ê³¡ ë„£ëŠ” ì—­í• 

#     for page in range(2, total_pages + 1):
#         params['pageNo'] = str(page) # í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ API ìš”ì²­ íŒŒë¼ë¯¸í„°ì— ì„¤ì •
#         response = requests.get(api_url, params=params, timeout=10) # í•´ë‹¹ í˜ì´ì§€ì— ëŒ€í•œ API ìš”ì²­ì „ì†¡
#         response.raise_for_status() # ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ ì˜ˆì™¸ ë°œìƒì‹œí‚´ (ì˜ˆ: 404, 500)
#         page_json = response.json() # í˜ì´ì§€ ì‘ë‹µ JSON íŒŒì‹±
#         docs = json_to_documents(page_json) # í•´ë‹¹ í˜ì´ì§€ì˜ JSONì„ Document ê°ì²´ë¡œ ë³€í™˜
#         all_documents.extend(docs) # ë³€í™˜ëœ ë¬¸ì„œë“¤ì„ all_documents ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
#         print(f"ğŸ“„ {page}/{total_pages} í˜ì´ì§€ ìˆ˜ì§‘ ì™„ë£Œ") 

#     print(f"\nâœ… ì´ {len(all_documents)}ê°œì˜ Document ê°ì²´ ìƒì„± ì™„ë£Œ")
#     return all_documents

def fetch_all_documents(api_url, api_key, num_of_rows=100, test_pages=1) -> list[Document]:
    all_documents = []
    params = {
        "ServiceKey": api_key,
        "pageNo": "1",
        "numOfRows": str(num_of_rows),
        "type": "json",
    }

    response = requests.get(api_url, params=params, timeout=10)
    response.raise_for_status()
    first_page = response.json()

    total_count = int(first_page['body']['totalCount'])
    total_pages = (total_count // num_of_rows) + (1 if total_count % num_of_rows else 0)

    # í…ŒìŠ¤íŠ¸: test_pages ë§Œí¼ë§Œ ìˆ˜ì§‘
    all_documents.extend(json_to_documents(first_page))

    for page in range(2, min(test_pages, total_pages) + 1):
        params['pageNo'] = str(page)
        response = requests.get(api_url, params=params, timeout=10)
        response.raise_for_status()
        page_json = response.json()
        docs = json_to_documents(page_json)
        all_documents.extend(docs)
        print(f"ğŸ“„ {page}/{total_pages} í˜ì´ì§€ ìˆ˜ì§‘ ì™„ë£Œ (í…ŒìŠ¤íŠ¸)")

    print(f"\nâœ… (í…ŒìŠ¤íŠ¸) ì´ {len(all_documents)}ê°œì˜ Document ê°ì²´ ìƒì„± ì™„ë£Œ")
    return all_documents

# ------------------------
# 3. ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ë¬¸ì„œ ì—…ë¡œë“œ
# ------------------------
# def build_vector_store(documents, index_name):
#     embeddings = OpenAIEmbeddings(model=os.environ['OPENAI_EMBEDDING_MODEL'])

#     # ë¬¸ì„œ ë¶„í• 
#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
#     split_documents = text_splitter.split_documents(documents)

#     # Pinecone ì´ˆê¸°í™” ë° ì¸ë±ìŠ¤ ì¤€ë¹„
#     pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])

#     if index_name not in pc.list_indexes().names():
#         pc.create_index(
#             name=index_name,
#             dimension=embeddings.get_dimension(),
#             metric='cosine'
#         )
#         while not pc.describe_index(index_name).status['ready']:
#             time.sleep(1)

#     vector_store = PineconeVectorStore(index_name=index_name, embedding=embeddings)

#     # ë°°ì¹˜ ì—…ë¡œë“œ
#     batch_size = 100
#     for i in range(0, len(split_documents), batch_size):
#         batch = split_documents[i:i + batch_size]
#         # vector_store.add_documents(batch)  # ì£¼ì„ í•´ì œ ì‹œ ì‹¤ì œ ì—…ë¡œë“œ ìˆ˜í–‰
#         print(f"Added batch {i//batch_size + 1}/{(len(split_documents)//batch_size) + 1}")

#     print("\nâœ… All documents added to Pinecone vector store.")
#     return vector_store
def build_vector_store(documents, index_name):
    embeddings = OpenAIEmbeddings(model=os.environ['OPENAI_EMBEDDING_MODEL'])
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_documents = text_splitter.split_documents(documents)

    pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])

    if index_name not in pc.list_indexes().names():
        pc.create_index(
            name=index_name,
            dimension=embeddings.get_dimension(),
            metric='cosine'
        )
        while not pc.describe_index(index_name).status['ready']:
            time.sleep(1)

    vector_store = PineconeVectorStore(index_name=index_name, embedding=embeddings)

    batch_size = 100
    for i in range(0, len(split_documents), batch_size):
        batch = split_documents[i:i + batch_size]
        # ì‹¤ì œ ì—…ë¡œë“œëŠ” ì£¼ì„ ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸)
        vector_store.add_documents(batch)
        print(f"(í…ŒìŠ¤íŠ¸) Added batch {i//batch_size + 1}/{(len(split_documents)//batch_size) + 1}")

    print("\nâœ… (í…ŒìŠ¤íŠ¸) All documents processed for Pinecone vector store.")
    return vector_store


# ------------------------
# 4. RAG ì§ˆì˜ ì‹œìŠ¤í…œ êµ¬ì„±
# ------------------------
def build_qa_chain(vector_store):
    retriever = vector_store.as_retriever(
        search_type='similarity',
        search_kwargs={'k': 3}
    )

    llm = ChatOpenAI(
        model_name="gpt-4.1-mini",
        temperature=0.3,
        max_tokens=512
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        return_source_documents=True
    )
    return qa_chain

# ------------------------
# 5. ë©”ì¸ ì‹¤í–‰ë¶€ ì˜ˆì‹œ
# ------------------------
# if __name__ == "__main__":
#     url = 'http://apis.data.go.kr/1471000/HtfsInfoService03/getHtfsItem01'
#     api_key = os.environ.get('OPENAI_API_KEY')

#     # Step 1: ë¬¸ì„œ ìˆ˜ì§‘
#     documents = fetch_all_documents(url, api_key)

#     # Step 2: ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ë¬¸ì„œ ì—…ë¡œë“œ
#     index_name = 'health-supplement-rag'
#     vector_store = build_vector_store(documents, index_name)

#     # Step 3: ì§ˆì˜ ì‘ë‹µ ì‹œìŠ¤í…œ êµ¬ì¶•
#     qa_chain = build_qa_chain(vector_store)

#     # Step 4: ì‚¬ìš©ì ì§ˆì˜ ì²˜ë¦¬
#     query = "í”¼ë¡œê°œì„ ì— ë„ì›€ì´ ë˜ëŠ” ì˜ì–‘ì œëŠ”?"
#     response = qa_chain(query)

#     print("\nğŸ§  GPT ì‘ë‹µ:")
#     print(response['result'])

#     print("\nğŸ“ ì°¸ê³  ë¬¸ì„œ:")
#     for doc in response['source_documents'][:3]:
#         print(f"ì œí’ˆëª…: {doc.metadata['ì œí’ˆëª…']} | ì œì¡°ì‚¬: {doc.metadata['ì œì¡°ì‚¬']} | ë“±ë¡ì¼ì: {doc.metadata['ë“±ë¡ì¼ì']}")
#         print(f"ì´ë¯¸ì§€URL: {doc.metadata.get('ì´ë¯¸ì§€URL', '')}")
#         print("-" * 60)

if __name__ == "__main__":
    url = 'http://apis.data.go.kr/1471000/HtfsInfoService03/getHtfsItem01'
    api_key = os.environ.get('OPENAI_API_KEY')

    # Step 1: ë¬¸ì„œ ìˆ˜ì§‘ (í…ŒìŠ¤íŠ¸: 2í˜ì´ì§€ë§Œ ìˆ˜ì§‘)
    documents = fetch_all_documents(url, api_key, num_of_rows=10, test_pages=2)

    # Step 2: ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ë¬¸ì„œ ì—…ë¡œë“œ (ì‹¤ì œ ì—…ë¡œë“œëŠ” ì£¼ì„ ì²˜ë¦¬)
    index_name = 'health-supplement-rag-test'
    vector_store = build_vector_store(documents, index_name)

    # Step 3: ì§ˆì˜ ì‘ë‹µ ì‹œìŠ¤í…œ êµ¬ì¶•
    qa_chain = build_qa_chain(vector_store)

    # Step 4: ì‚¬ìš©ì ì§ˆì˜ ì²˜ë¦¬
    query = "í”¼ë¡œê°œì„ ì— ë„ì›€ì´ ë˜ëŠ” ì˜ì–‘ì œëŠ”?"
    response = qa_chain(query)

    print("\nğŸ§  GPT ì‘ë‹µ:")
    print(response['result'])

    print("\nğŸ“ ì°¸ê³  ë¬¸ì„œ:")
    for doc in response['source_documents'][:3]:
        print(f"ì œí’ˆëª…: {doc.metadata['ì œí’ˆëª…']} | ì œì¡°ì‚¬: {doc.metadata['ì œì¡°ì‚¬']} | ë“±ë¡ì¼ì: {doc.metadata['ë“±ë¡ì¼ì']}")
        print(f"ì´ë¯¸ì§€URL: {doc.metadata.get('ì´ë¯¸ì§€URL', '')}")
        print("-" * 60)


# ë„¤ì´ë²„ ì´ë¯¸ì§€ ê²€ìƒ‰ API í˜¸ì¶œ í•¨ìˆ˜
def fetch_image_url(product_name, client_id, client_secret):
    import urllib.parse

    base_url = "https://openapi.naver.com/v1/search/image"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }
    params = {
        "query": product_name,
        "display": 1,
        "sort": "sim"
    }
    response = requests.get(base_url, headers=headers, params=params, timeout=5)
    if response.status_code == 200:
        items = response.json().get("items")
        if items:
            return items[0].get("link")
    return ""

